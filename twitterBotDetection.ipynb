{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b387e684",
   "metadata": {},
   "source": [
    "Cell 1 – Imports and basic config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acd06d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility helper\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773ab60a",
   "metadata": {},
   "source": [
    "Cell 2 – Paths & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8c0ef34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: mlp_256_128_64\n",
      "Ensemble seeds: [42, 7, 2024]\n"
     ]
    }
   ],
   "source": [
    "# Adjust these paths if needed\n",
    "PROFILE_FEATURES_CSV = \"./twibot_user_features_clean.csv\"   # contains profile + label + split\n",
    "GRAPH_FEATURES_CSV   = \"./graph_features.csv\"               # contains graph features per user id\n",
    "\n",
    "# Output artifact paths\n",
    "SCALER_PATH          = \"./twibot_scaler.pkl\"\n",
    "ENSEMBLE_CKPT_PATH   = \"./twibot_mlp_ensemble.pt\"\n",
    "RESULTS_JSON_PATH    = \"./twibot_mlp_results.json\"\n",
    "\n",
    "# Best architecture we chose from previous experiments\n",
    "ARCHITECTURE = \"mlp_256_128_64\"\n",
    "ENSEMBLE_SEEDS = [42, 7, 2024]\n",
    "\n",
    "print(\"Architecture:\", ARCHITECTURE)\n",
    "print(\"Ensemble seeds:\", ENSEMBLE_SEEDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b46250",
   "metadata": {},
   "source": [
    "Cell 3 – Load data & basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46f84ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Profile features shape: (990913, 18)\n",
      "   followers_count  following_count  tweet_count   ff_ratio  \\\n",
      "0             7316              215         3098  33.870370   \n",
      "1              123             1090         1823   0.112741   \n",
      "2                3               62           66   0.047619   \n",
      "3              350              577          237   0.605536   \n",
      "4              240              297         3713   0.805369   \n",
      "\n",
      "   tweets_per_following  tweets_per_follower  desc_length  has_url  verified  \\\n",
      "0             14.342593             0.423398           92        1         0   \n",
      "1              1.670944            14.701613           10        0         0   \n",
      "2              1.047619            16.500000            1        0         0   \n",
      "3              0.410035             0.675214          127        0         0   \n",
      "4             12.459732            15.406639           35        0         0   \n",
      "\n",
      "   account_age_days  log_followers_count  log_following_count  \\\n",
      "0       2132.706977             8.897956             5.375278   \n",
      "1       4156.044582             4.820282             6.994850   \n",
      "2       1997.284871             1.386294             4.143135   \n",
      "3       2487.213991             5.860786             6.359574   \n",
      "4       6044.999489             5.484797             5.697093   \n",
      "\n",
      "   log_tweet_count  log_listed_count                    id  label  split  \\\n",
      "0         8.038835          4.248495  u1217628182611927040  human   test   \n",
      "1         7.508787          0.000000           u2664730894  human  train   \n",
      "2         4.204693          0.000000  u1266703520205549568  human   test   \n",
      "3         5.472271          0.693147  u1089159225148882949  human  train   \n",
      "4         8.219865          2.197225             u36741729    bot  train   \n",
      "\n",
      "   account_age_years  \n",
      "0           5.843033  \n",
      "1          11.386424  \n",
      "2           5.472013  \n",
      "3           6.814285  \n",
      "4          16.561642  \n",
      "\n",
      "Label distribution (from profile_df):\n",
      "label\n",
      "human    854940\n",
      "bot      135973\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Split distribution (from profile_df):\n",
      "split\n",
      "train    692925\n",
      "val      199108\n",
      "test      98880\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[*] Graph features shape: (1000000, 12)\n",
      "                     id  out_following  in_following  out_followers  \\\n",
      "0  u1217628182611927040            202           127            316   \n",
      "1           u2664730894            581             4             55   \n",
      "2  u1266703520205549568             59             1              3   \n",
      "3  u1089159225148882949            279             2            114   \n",
      "4             u36741729              0             0              0   \n",
      "\n",
      "   in_followers  out_mentioned  in_mentioned  out_retweeted  in_retweeted  \\\n",
      "0             5              0             0              0             0   \n",
      "1             3              0             0              0             0   \n",
      "2             1              0             0              0             0   \n",
      "3             4              0             0              0             0   \n",
      "4             1              0             0              0             0   \n",
      "\n",
      "   deg_follow  deg_mention  deg_retweet  \n",
      "0         650            0            0  \n",
      "1         643            0            0  \n",
      "2          64            0            0  \n",
      "3         399            0            0  \n",
      "4           1            0            0  \n",
      "\n",
      "[*] After merging profile + graph, shape: (990913, 29)\n",
      "\n",
      "[*] Final available splits after merge:\n",
      "split\n",
      "train    692925\n",
      "val      199108\n",
      "test      98880\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[*] Basic stats preview (first N numeric columns):\n",
      "       followers_count  following_count     tweet_count       ff_ratio  \\\n",
      "count    990913.000000    990913.000000   990913.000000  990913.000000   \n",
      "mean      18465.133735      2006.145696    21358.598104       7.988808   \n",
      "std       69332.855014      6411.937179    61848.239577      15.414033   \n",
      "min           0.000000         0.000000        0.000000       0.000000   \n",
      "25%         165.000000       214.000000      395.000000       0.376374   \n",
      "50%         955.000000       653.000000     3220.000000       1.050089   \n",
      "75%        5137.000000      1723.000000    15637.000000       4.529964   \n",
      "max      500000.000000    100000.000000  1000000.000000      50.000000   \n",
      "\n",
      "       tweets_per_following  tweets_per_follower    desc_length  \\\n",
      "count         990913.000000        990913.000000  990913.000000   \n",
      "mean              42.978410            19.213611      88.741976   \n",
      "std              147.338484            72.991116      56.262463   \n",
      "min                0.000000             0.000000       0.000000   \n",
      "25%                0.855738             0.544734      36.000000   \n",
      "50%                4.216374             2.204444      98.000000   \n",
      "75%               17.778626             9.452489     143.000000   \n",
      "max             1000.000000          1000.000000     317.000000   \n",
      "\n",
      "             has_url       verified  account_age_days  \n",
      "count  990913.000000  990913.000000     990913.000000  \n",
      "mean        0.521550       0.096273       4049.752758  \n",
      "std         0.499536       0.294965       1624.173146  \n",
      "min         0.000000       0.000000       1345.264356  \n",
      "25%         0.000000       0.000000       2487.468581  \n",
      "50%         1.000000       0.000000       4260.012181  \n",
      "75%         1.000000       0.000000       5537.057843  \n",
      "max         1.000000       1.000000      20409.792443  \n"
     ]
    }
   ],
   "source": [
    "# Load profile features (already has label + split columns)\n",
    "profile_df = pd.read_csv(PROFILE_FEATURES_CSV)\n",
    "print(\"[*] Profile features shape:\", profile_df.shape)\n",
    "print(profile_df.head())\n",
    "\n",
    "# Optional: quick label + split distribution from this single merged CSV\n",
    "print(\"\\nLabel distribution (from profile_df):\")\n",
    "print(profile_df[\"label\"].value_counts())\n",
    "\n",
    "print(\"\\nSplit distribution (from profile_df):\")\n",
    "print(profile_df[\"split\"].value_counts())\n",
    "\n",
    "# Load graph features (must share 'id' column with profile_df)\n",
    "graph_df = pd.read_csv(GRAPH_FEATURES_CSV)\n",
    "print(\"\\n[*] Graph features shape:\", graph_df.shape)\n",
    "print(graph_df.head())\n",
    "\n",
    "# Merge on 'id' to build final feature dataframe\n",
    "feat_df = profile_df.merge(graph_df, on=\"id\", how=\"inner\")\n",
    "print(\"\\n[*] After merging profile + graph, shape:\", feat_df.shape)\n",
    "\n",
    "# Keep only rows with split in {train, val, test} for safety\n",
    "feat_df = feat_df[feat_df[\"split\"].isin([\"train\", \"val\", \"test\"])].copy()\n",
    "\n",
    "print(\"\\n[*] Final available splits after merge:\")\n",
    "print(feat_df[\"split\"].value_counts())\n",
    "\n",
    "# Show basic stats for first few numeric columns\n",
    "print(\"\\n[*] Basic stats preview (first N numeric columns):\")\n",
    "print(feat_df.describe().iloc[:, :10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f70d6",
   "metadata": {},
   "source": [
    "Cell 4 – Prepare features, labels, and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8fced96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Final feature columns (26):\n",
      "['followers_count', 'following_count', 'tweet_count', 'ff_ratio', 'tweets_per_following', 'tweets_per_follower', 'desc_length', 'has_url', 'verified', 'account_age_days', 'log_followers_count', 'log_following_count', 'log_tweet_count', 'log_listed_count', 'account_age_years', 'out_following', 'in_following', 'out_followers', 'in_followers', 'out_mentioned', 'in_mentioned', 'out_retweeted', 'in_retweeted', 'deg_follow', 'deg_mention', 'deg_retweet']\n",
      "\n",
      "[*] Label mapping check:\n",
      "label  y\n",
      "human  0    854940\n",
      "bot    1    135973\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[*] Split sizes:\n",
      "  train: (692925, 30)\n",
      "  val  : (199108, 30)\n",
      "  test : (98880, 30)\n"
     ]
    }
   ],
   "source": [
    "# Columns that are NOT features\n",
    "NON_FEATURE_COLS = [\"id\", \"label\", \"split\"]\n",
    "\n",
    "# Automatically infer feature columns\n",
    "feature_cols = [c for c in feat_df.columns if c not in NON_FEATURE_COLS]\n",
    "print(\"\\n[*] Final feature columns ({}):\".format(len(feature_cols)))\n",
    "print(feature_cols)\n",
    "\n",
    "# Encode labels: assume 'bot' vs 'human'\n",
    "label_map = {\"human\": 0, \"bot\": 1}\n",
    "feat_df[\"y\"] = feat_df[\"label\"].map(label_map)\n",
    "\n",
    "# Sanity check\n",
    "print(\"\\n[*] Label mapping check:\")\n",
    "print(feat_df[[\"label\", \"y\"]].value_counts().head())\n",
    "\n",
    "# Split by 'split' column\n",
    "train_df = feat_df[feat_df[\"split\"] == \"train\"].copy()\n",
    "val_df   = feat_df[feat_df[\"split\"] == \"val\"].copy()\n",
    "test_df  = feat_df[feat_df[\"split\"] == \"test\"].copy()\n",
    "\n",
    "print(\"\\n[*] Split sizes:\")\n",
    "print(\"  train:\", train_df.shape)\n",
    "print(\"  val  :\", val_df.shape)\n",
    "print(\"  test :\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a53ba2",
   "metadata": {},
   "source": [
    "Cell 5 – Handle class imbalance (undersampling + pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d005d5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Raw TRAIN label distribution (before rebalancing):\n",
      "y\n",
      "0    641040\n",
      "1     51885\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[*] TRAIN label distribution AFTER rebalancing:\n",
      "y\n",
      "0    103770\n",
      "1     51885\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[*] Shapes before scaling:\n",
      "  X_train: (155655, 26) y_train: (155655,)\n",
      "  X_val  : (199108, 26) y_val  : (199108,)\n",
      "  X_test : (98880, 26) y_test : (98880,)\n",
      "\n",
      "[*] pos_weight (for bot class) in BCEWithLogitsLoss: 2.0\n"
     ]
    }
   ],
   "source": [
    "# class imbalance handling (undersample majority class + pos_weight)\n",
    "\n",
    "# Count class distribution in raw train split\n",
    "train_counts = train_df[\"y\"].value_counts()\n",
    "num_humans = int(train_counts.get(0, 0))\n",
    "num_bots   = int(train_counts.get(1, 0))\n",
    "\n",
    "print(\"\\n[*] Raw TRAIN label distribution (before rebalancing):\")\n",
    "print(train_counts)\n",
    "\n",
    "# Simple strategy: undersample humans to ~2x bots (same as earlier code)\n",
    "# If bots=~51k, humans_sampled=~103k\n",
    "target_humans = min(num_humans, 2 * num_bots)\n",
    "\n",
    "humans_df = train_df[train_df[\"y\"] == 0]\n",
    "bots_df   = train_df[train_df[\"y\"] == 1]\n",
    "\n",
    "humans_sampled = humans_df.sample(n=target_humans, random_state=42)\n",
    "train_rebalanced_df = pd.concat([humans_sampled, bots_df], axis=0).sample(frac=1.0, random_state=42)\n",
    "\n",
    "print(\"\\n[*] TRAIN label distribution AFTER rebalancing:\")\n",
    "print(train_rebalanced_df[\"y\"].value_counts())\n",
    "\n",
    "# Build X, y for train/val/test\n",
    "X_train = train_rebalanced_df[feature_cols].values\n",
    "y_train = train_rebalanced_df[\"y\"].values\n",
    "\n",
    "X_val   = val_df[feature_cols].values\n",
    "y_val   = val_df[\"y\"].values\n",
    "\n",
    "X_test  = test_df[feature_cols].values\n",
    "y_test  = test_df[\"y\"].values\n",
    "\n",
    "print(\"\\n[*] Shapes before scaling:\")\n",
    "print(\"  X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"  X_val  :\", X_val.shape,   \"y_val  :\", y_val.shape)\n",
    "print(\"  X_test :\", X_test.shape,  \"y_test :\", y_test.shape)\n",
    "\n",
    "# pos_weight = (#neg / #pos) = humans / bots (on rebalanced train)\n",
    "num_humans_reb = (y_train == 0).sum()\n",
    "num_bots_reb   = (y_train == 1).sum()\n",
    "\n",
    "pos_weight_value = num_humans_reb / num_bots_reb\n",
    "print(\"\\n[*] pos_weight (for bot class) in BCEWithLogitsLoss:\", round(pos_weight_value, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6511796",
   "metadata": {},
   "source": [
    "Cell 6 – Standardize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1456d712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Scaling done (StandardScaler fitted on TRAIN).\n",
      "[*] Scaler saved to: ./twibot_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: standard scaling (fit on TRAIN only)\n",
    "\n",
    "import joblib \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n[*] Scaling done (StandardScaler fitted on TRAIN).\")\n",
    "\n",
    "# Save scaler for later use (deployment / demo)\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "print(\"[*] Scaler saved to:\", SCALER_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab80e52",
   "metadata": {},
   "source": [
    "Cell 7 – MLP model, train loop, eval helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1cc443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Define MLP model, training loop, and evaluation utilities\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Flexible MLP with given hidden_dims list.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(last_dim, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            last_dim = h\n",
    "        layers.append(nn.Linear(last_dim, 1))  # binary logit output\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "def train_one_model(hidden_dims,\n",
    "                    seed=42,\n",
    "                    lr=1e-3,\n",
    "                    max_epochs=60,\n",
    "                    patience=10,\n",
    "                    batch_size=4096):\n",
    "    \"\"\"Train one MLP with a given hidden_dims and random seed.\"\"\"\n",
    "    set_seed(seed)\n",
    "\n",
    "    train_loader, val_loader, test_loader = make_dataloaders(batch_size=batch_size)\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "\n",
    "    model = MLP(input_dim=input_dim, hidden_dims=hidden_dims).to(DEVICE)\n",
    "\n",
    "    # pos_weight for bots (label 1) to handle imbalance\n",
    "    num_pos = (y_train == 1).sum()\n",
    "    num_neg = (y_train == 0).sum()\n",
    "    pos_weight = torch.tensor(num_neg / num_pos, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_f1 = -1.0\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # ----- Train -----\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE).float()  # BCEWithLogitsLoss expects float for targets\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = float(np.mean(train_losses))\n",
    "\n",
    "        # ----- Validate -----\n",
    "        model.eval()\n",
    "        all_val_logits = []\n",
    "        all_val_labels = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                all_val_logits.append(logits.cpu().numpy())\n",
    "                all_val_labels.append(yb.numpy())\n",
    "\n",
    "        val_logits = np.concatenate(all_val_logits)\n",
    "        val_labels = np.concatenate(all_val_labels)\n",
    "\n",
    "        # Default threshold 0.5 for early stopping metric\n",
    "        val_probs = 1.0 / (1.0 + np.exp(-val_logits))\n",
    "        val_preds = (val_probs >= 0.5).astype(int)\n",
    "\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_macro_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | TrainLoss={avg_train_loss:.4f} | \"\n",
    "              f\"ValAcc={val_acc:.4f} | ValMacroF1={val_macro_f1:.4f}\")\n",
    "\n",
    "        # Track best model by VAL macro F1\n",
    "        if val_macro_f1 > best_val_f1 + 1e-4:\n",
    "            best_val_f1 = val_macro_f1\n",
    "            best_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"[*] Early stopping triggered at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "    # Restore best model\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, best_val_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a0eb7",
   "metadata": {},
   "source": [
    "Cell 8 – Threshold tuning + final evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3ddd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Threshold search on VAL and final TEST evaluation\n",
    "\n",
    "def find_best_threshold(model, val_loader, thresholds=None):\n",
    "    \"\"\"Search threshold on validation to maximize macro F1.\"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.5, 0.9, 41)  # 0.50, 0.51, ..., 0.90\n",
    "\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_labels.append(yb.numpy())\n",
    "\n",
    "    logits = np.concatenate(all_logits)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "\n",
    "    best_thr = 0.5\n",
    "    best_f1 = -1.0\n",
    "\n",
    "    for thr in thresholds:\n",
    "        preds = (probs >= thr).astype(int)\n",
    "        f1 = f1_score(labels, preds, average=\"macro\")\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = thr\n",
    "\n",
    "    return best_thr, best_f1\n",
    "\n",
    "\n",
    "def evaluate_on_loader(model, loader, threshold):\n",
    "    \"\"\"Evaluate model on a DataLoader using given threshold.\"\"\"\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_labels.append(yb.numpy())\n",
    "\n",
    "    logits = np.concatenate(all_logits)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    report = classification_report(labels, preds, target_names=[\"Human\", \"Bot\"])\n",
    "\n",
    "    return acc, macro_f1, report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298d78d",
   "metadata": {},
   "source": [
    "Cell 9 – Train with best architecture (mlp_256_128_64) using 3 seeds and simple ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9fc1e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      " Training best architecture: mlp_256_128_64\n",
      "===========================================\n",
      "\n",
      "============================\n",
      " Training model with seed=42\n",
      "============================\n",
      "Epoch 01 | TrainLoss=0.6768 | ValAcc=0.5596 | ValMacroF1=0.5531\n",
      "Epoch 02 | TrainLoss=0.6669 | ValAcc=0.5614 | ValMacroF1=0.5549\n",
      "Epoch 03 | TrainLoss=0.6161 | ValAcc=0.5445 | ValMacroF1=0.5405\n",
      "Epoch 04 | TrainLoss=0.6244 | ValAcc=0.5612 | ValMacroF1=0.5544\n",
      "Epoch 05 | TrainLoss=0.6113 | ValAcc=0.5577 | ValMacroF1=0.5514\n",
      "Epoch 06 | TrainLoss=0.6140 | ValAcc=0.5542 | ValMacroF1=0.5487\n",
      "Epoch 07 | TrainLoss=0.6216 | ValAcc=0.5628 | ValMacroF1=0.5557\n",
      "Epoch 08 | TrainLoss=0.6095 | ValAcc=0.5675 | ValMacroF1=0.5597\n",
      "Epoch 09 | TrainLoss=0.6080 | ValAcc=0.5500 | ValMacroF1=0.5452\n",
      "Epoch 10 | TrainLoss=0.6061 | ValAcc=0.5712 | ValMacroF1=0.5625\n",
      "Epoch 11 | TrainLoss=0.6090 | ValAcc=0.5575 | ValMacroF1=0.5516\n",
      "Epoch 12 | TrainLoss=0.6079 | ValAcc=0.5944 | ValMacroF1=0.5803\n",
      "Epoch 13 | TrainLoss=0.6102 | ValAcc=0.5621 | ValMacroF1=0.5550\n",
      "Epoch 14 | TrainLoss=0.6141 | ValAcc=0.5502 | ValMacroF1=0.5454\n",
      "Epoch 15 | TrainLoss=0.6093 | ValAcc=0.5516 | ValMacroF1=0.5465\n",
      "Epoch 16 | TrainLoss=0.6096 | ValAcc=0.5704 | ValMacroF1=0.5618\n",
      "Epoch 17 | TrainLoss=0.6198 | ValAcc=0.5774 | ValMacroF1=0.5673\n",
      "Epoch 18 | TrainLoss=0.6076 | ValAcc=0.5840 | ValMacroF1=0.5727\n",
      "Epoch 19 | TrainLoss=0.6008 | ValAcc=0.5711 | ValMacroF1=0.5621\n",
      "Epoch 20 | TrainLoss=0.6107 | ValAcc=0.5534 | ValMacroF1=0.5482\n",
      "Epoch 21 | TrainLoss=0.5985 | ValAcc=0.5849 | ValMacroF1=0.5728\n",
      "Epoch 22 | TrainLoss=0.6370 | ValAcc=0.5818 | ValMacroF1=0.5707\n",
      "[*] Early stopping triggered at epoch 22.\n",
      "[*] Best VAL macro F1 for seed 42: 0.5803\n",
      "\n",
      "============================\n",
      " Training model with seed=7\n",
      "============================\n",
      "Epoch 01 | TrainLoss=0.6742 | ValAcc=0.5160 | ValMacroF1=0.5150\n",
      "Epoch 02 | TrainLoss=0.6338 | ValAcc=0.5356 | ValMacroF1=0.5330\n",
      "Epoch 03 | TrainLoss=0.6165 | ValAcc=0.5614 | ValMacroF1=0.5547\n",
      "Epoch 04 | TrainLoss=0.6164 | ValAcc=0.5165 | ValMacroF1=0.5153\n",
      "Epoch 05 | TrainLoss=0.6139 | ValAcc=0.5619 | ValMacroF1=0.5552\n",
      "Epoch 06 | TrainLoss=0.6141 | ValAcc=0.5358 | ValMacroF1=0.5331\n",
      "Epoch 07 | TrainLoss=0.6224 | ValAcc=0.5533 | ValMacroF1=0.5480\n",
      "Epoch 08 | TrainLoss=0.6206 | ValAcc=0.5457 | ValMacroF1=0.5417\n",
      "Epoch 09 | TrainLoss=0.6132 | ValAcc=0.5583 | ValMacroF1=0.5522\n",
      "Epoch 10 | TrainLoss=0.6109 | ValAcc=0.5478 | ValMacroF1=0.5434\n",
      "Epoch 11 | TrainLoss=0.6047 | ValAcc=0.5538 | ValMacroF1=0.5483\n",
      "Epoch 12 | TrainLoss=0.6373 | ValAcc=0.5898 | ValMacroF1=0.5766\n",
      "Epoch 13 | TrainLoss=0.6197 | ValAcc=0.5778 | ValMacroF1=0.5675\n",
      "Epoch 14 | TrainLoss=0.6107 | ValAcc=0.5791 | ValMacroF1=0.5687\n",
      "Epoch 15 | TrainLoss=0.6068 | ValAcc=0.5548 | ValMacroF1=0.5491\n",
      "Epoch 16 | TrainLoss=0.5991 | ValAcc=0.5536 | ValMacroF1=0.5483\n",
      "Epoch 17 | TrainLoss=0.6010 | ValAcc=0.5542 | ValMacroF1=0.5489\n",
      "Epoch 18 | TrainLoss=0.6034 | ValAcc=0.5720 | ValMacroF1=0.5626\n",
      "Epoch 19 | TrainLoss=0.6119 | ValAcc=0.5664 | ValMacroF1=0.5588\n",
      "Epoch 20 | TrainLoss=0.6057 | ValAcc=0.5874 | ValMacroF1=0.5750\n",
      "Epoch 21 | TrainLoss=0.6042 | ValAcc=0.5750 | ValMacroF1=0.5655\n",
      "Epoch 22 | TrainLoss=0.6187 | ValAcc=0.5703 | ValMacroF1=0.5616\n",
      "[*] Early stopping triggered at epoch 22.\n",
      "[*] Best VAL macro F1 for seed 7: 0.5766\n",
      "\n",
      "============================\n",
      " Training model with seed=2024\n",
      "============================\n",
      "Epoch 01 | TrainLoss=0.6767 | ValAcc=0.5350 | ValMacroF1=0.5320\n",
      "Epoch 02 | TrainLoss=0.6330 | ValAcc=0.5391 | ValMacroF1=0.5359\n",
      "Epoch 03 | TrainLoss=0.6229 | ValAcc=0.5568 | ValMacroF1=0.5509\n",
      "Epoch 04 | TrainLoss=0.6089 | ValAcc=0.5346 | ValMacroF1=0.5319\n",
      "Epoch 05 | TrainLoss=0.6138 | ValAcc=0.5529 | ValMacroF1=0.5479\n",
      "Epoch 06 | TrainLoss=0.6208 | ValAcc=0.5742 | ValMacroF1=0.5648\n",
      "Epoch 07 | TrainLoss=0.6081 | ValAcc=0.5495 | ValMacroF1=0.5448\n",
      "Epoch 08 | TrainLoss=0.6257 | ValAcc=0.5611 | ValMacroF1=0.5545\n",
      "Epoch 09 | TrainLoss=0.6177 | ValAcc=0.5951 | ValMacroF1=0.5804\n",
      "Epoch 10 | TrainLoss=0.6166 | ValAcc=0.5672 | ValMacroF1=0.5596\n",
      "Epoch 11 | TrainLoss=0.6039 | ValAcc=0.5483 | ValMacroF1=0.5439\n",
      "Epoch 12 | TrainLoss=0.6009 | ValAcc=0.5579 | ValMacroF1=0.5518\n",
      "Epoch 13 | TrainLoss=0.6321 | ValAcc=0.5464 | ValMacroF1=0.5422\n",
      "Epoch 14 | TrainLoss=0.6046 | ValAcc=0.5735 | ValMacroF1=0.5641\n",
      "Epoch 15 | TrainLoss=0.5996 | ValAcc=0.5753 | ValMacroF1=0.5655\n",
      "Epoch 16 | TrainLoss=0.6091 | ValAcc=0.5569 | ValMacroF1=0.5510\n",
      "Epoch 17 | TrainLoss=0.5973 | ValAcc=0.5617 | ValMacroF1=0.5545\n",
      "Epoch 18 | TrainLoss=0.6083 | ValAcc=0.5370 | ValMacroF1=0.5341\n",
      "Epoch 19 | TrainLoss=0.5946 | ValAcc=0.5856 | ValMacroF1=0.5734\n",
      "[*] Early stopping triggered at epoch 19.\n",
      "[*] Best VAL macro F1 for seed 2024: 0.5804\n",
      "\n",
      "[*] VAL macro F1 per seed: {42: 0.580267544755271, 7: 0.576597554590006, 2024: 0.5803697978822412}\n",
      "\n",
      "[*] Best threshold on VAL (macro F1): 0.7700\n",
      "[*] Best VAL macro F1 at that threshold: 0.6352\n",
      "\n",
      "TEST Accuracy: 0.7203 (threshold=0.7700)\n",
      "TEST Macro F1: 0.6662\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       0.81      0.79      0.80     70165\n",
      "         Bot       0.52      0.55      0.53     28715\n",
      "\n",
      "    accuracy                           0.72     98880\n",
      "   macro avg       0.66      0.67      0.67     98880\n",
      "weighted avg       0.73      0.72      0.72     98880\n",
      "\n",
      "\n",
      "[*] Saved summary to: D:\\Codes\\ML\\TwiBot-22\\twibot_best_mlp_results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Train best architecture (mlp_256_128_64) with multiple seeds and ensemble\n",
    "\n",
    "BEST_ARCH_NAME = \"mlp_256_128_64\"\n",
    "BEST_HIDDEN_DIMS = [256, 128, 64]\n",
    "SEEDS = [42, 7, 2024]\n",
    "\n",
    "print(f\"===========================================\")\n",
    "print(f\" Training best architecture: {BEST_ARCH_NAME}\")\n",
    "print(f\"===========================================\")\n",
    "\n",
    "models = []\n",
    "val_f1_per_seed = {}\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\n============================\")\n",
    "    print(f\" Training model with seed={seed}\")\n",
    "    print(f\"============================\")\n",
    "    model, best_val_f1 = train_one_model(\n",
    "        hidden_dims=BEST_HIDDEN_DIMS,\n",
    "        seed=seed,\n",
    "        lr=1e-3,\n",
    "        max_epochs=60,\n",
    "        patience=10,\n",
    "        batch_size=4096,\n",
    "    )\n",
    "    models.append(model)\n",
    "    val_f1_per_seed[seed] = best_val_f1\n",
    "    print(f\"[*] Best VAL macro F1 for seed {seed}: {best_val_f1:.4f}\")\n",
    "\n",
    "print(\"\\n[*] VAL macro F1 per seed:\", val_f1_per_seed)\n",
    "\n",
    "# Build a simple ensemble that averages logits from all 3 models\n",
    "class EnsembleWrapper(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = []\n",
    "        for m in self.models:\n",
    "            logits.append(m(x))\n",
    "        stacked = torch.stack(logits, dim=0)  # [n_models, batch_size]\n",
    "        return stacked.mean(dim=0)  # average logits\n",
    "\n",
    "\n",
    "# Freeze ensemble models (no further training)\n",
    "for m in models:\n",
    "    m.eval()\n",
    "ensemble_model = EnsembleWrapper(models).to(DEVICE)\n",
    "\n",
    "# Get loaders\n",
    "_, val_loader, test_loader = make_dataloaders(batch_size=4096)\n",
    "\n",
    "# Threshold search on VAL for ensemble\n",
    "best_thr, best_val_macro_f1 = find_best_threshold(ensemble_model, val_loader)\n",
    "print(f\"\\n[*] Best threshold on VAL (macro F1): {best_thr:.4f}\")\n",
    "print(f\"[*] Best VAL macro F1 at that threshold: {best_val_macro_f1:.4f}\")\n",
    "\n",
    "# Final evaluation on TEST\n",
    "test_acc, test_macro_f1, test_report = evaluate_on_loader(\n",
    "    ensemble_model, test_loader, threshold=best_thr\n",
    ")\n",
    "\n",
    "print(f\"\\nTEST Accuracy: {test_acc:.4f} (threshold={best_thr:.4f})\")\n",
    "print(f\"TEST Macro F1: {test_macro_f1:.4f}\")\n",
    "print(\"\\nClassification report (TEST):\")\n",
    "print(test_report)\n",
    "\n",
    "# Optional: Save summary JSON for your paper/report\n",
    "results_summary = {\n",
    "    \"architecture\": BEST_ARCH_NAME,\n",
    "    \"hidden_dims\": BEST_HIDDEN_DIMS,\n",
    "    \"seeds\": SEEDS,\n",
    "    \"val_macro_f1_per_seed\": val_f1_per_seed,\n",
    "    \"best_val_macro_f1_ensemble\": float(best_val_macro_f1),\n",
    "    \"best_threshold\": float(best_thr),\n",
    "    \"test_accuracy\": float(test_acc),\n",
    "    \"test_macro_f1\": float(test_macro_f1),\n",
    "}\n",
    "\n",
    "out_path = os.path.join(DATA_DIR, \"twibot_best_mlp_results.json\")\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n[*] Saved summary to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a17ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9f10b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: helper to get probabilities and predictions from the ensemble\n",
    "\n",
    "def ensemble_predict_proba(models_dict, X_np, batch_size=4096):\n",
    "    \"\"\"Return averaged bot probability from ensemble for given features.\"\"\"\n",
    "    models = list(models_dict.values())\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "\n",
    "    X_tensor = torch.from_numpy(X_np).float().to(device)\n",
    "    probs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for m in models:\n",
    "            logits = []\n",
    "            for i in range(0, X_tensor.size(0), batch_size):\n",
    "                batch = X_tensor[i:i+batch_size]\n",
    "                batch_logits = m(batch).squeeze(1)\n",
    "                logits.append(batch_logits.cpu().numpy())\n",
    "            logits = np.concatenate(logits)\n",
    "            probs = 1 / (1 + np.exp(-logits))  # sigmoid\n",
    "            probs_list.append(probs)\n",
    "\n",
    "    avg_probs = np.mean(np.stack(probs_list, axis=0), axis=0)\n",
    "    return avg_probs  # probability of class \"bot\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51313d0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c9f0d3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# VAL\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m val_bot_probs = ensemble_predict_proba(\u001b[43mbest_models\u001b[49m, X_val)\n\u001b[32m      6\u001b[39m val_preds = (val_bot_probs >= best_threshold).astype(\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# 1=bot, 0=human\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== VALIDATION SET ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'best_models' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell: confusion matrices and classification reports\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# VAL\n",
    "val_bot_probs = ensemble_predict_proba(best_models, X_val)\n",
    "val_preds = (val_bot_probs >= best_threshold).astype(int)  # 1=bot, 0=human\n",
    "\n",
    "print(\"=== VALIDATION SET ===\")\n",
    "print(classification_report(y_val, val_preds, target_names=[\"Human\", \"Bot\"]))\n",
    "\n",
    "cm_val = confusion_matrix(y_val, val_preds)\n",
    "\n",
    "# TEST\n",
    "test_bot_probs = ensemble_predict_proba(best_models, X_test)\n",
    "test_preds = (test_bot_probs >= best_threshold).astype(int)\n",
    "\n",
    "print(\"\\n=== TEST SET ===\")\n",
    "print(classification_report(y_test, test_preds, target_names=[\"Human\", \"Bot\"]))\n",
    "\n",
    "cm_test = confusion_matrix(y_test, test_preds)\n",
    "\n",
    "print(\"\\nVAL Confusion Matrix:\\n\", cm_val)\n",
    "print(\"\\nTEST Confusion Matrix:\\n\", cm_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
